{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network \n",
    "1. Ustawienie losowych wag i biasów.\n",
    "2. Przepuszenie otrzymanych wyników przez funkcje aktywacji aby dodać nielinowość.\n",
    "3. Obliczenie błędu przewidywań wyników.\n",
    "4. Uruchomienie propagacji wstecznej aby poprawić skuteczność przewidywanych wyników.\n",
    "5. Trening modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight_bias():\n",
    "    weights = []\n",
    "    biases = []\n",
    "    \n",
    "    for _ in range(3):\n",
    "        weights.append(np.random.rand(2, 2) - 0.5)\n",
    "        biases.append(np.random.rand(1, 2) - 0.5)\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyczytałem, że dobrze jest odjąć 0.5 od losowo wygenerowanej liczby bo stwarza to zakrec który zawiera liczby dodatnie jak i ujemne oraz wartości unikalne które sprzyjają lepszemu uczeniu się modelowi i nie powodują uczenia się w ten sam sposób na tych samych wagach i odchyleniach. Jest to optymalizacja problemu symmetry breaking. Odjęcie od bias wartości 0.5 pomaga w uniknięciu stronniczości w uczeniu maszynowym, jest to podobno standardowa inicjalizacja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losowe wagi:  [array([[-0.03540673,  0.07771164],\n",
      "       [ 0.27841192,  0.21259358]]), array([[ 0.35593504, -0.47568224],\n",
      "       [-0.25641264,  0.33219778]]), array([[ 0.28354058, -0.40515167],\n",
      "       [ 0.38376576,  0.01795235]])]\n",
      "Losowe biasy:  [array([[-0.48383124, -0.09765069]]), array([[0.46089425, 0.48031995]]), array([[0.08582514, 0.12656794]])]\n"
     ]
    }
   ],
   "source": [
    "weights, biases = init_weight_bias()\n",
    "\n",
    "print(\"Losowe wagi: \", weights)\n",
    "print(\"Losowe biasy: \", biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/warstwa.png \"Layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstrakcyjna wartwa bazowa\n",
    "def layer(input_X, output_Y):\n",
    "    input_X = None\n",
    "    output_Y = None\n",
    "\n",
    "    return input_X, output_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/sigmoid.png \"funkcja aktywacji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdecydowałem się na funkcje aktywacji sigmoid ponieważ wyczytałem, że jest idealna do klasyfkikacji binarnej 0,1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/softmax.png \"softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Miałem nadzieję, że funkcja softmax dedykowana do używania na wyjściowej warstwie poprawi wyniki mojego modelu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/propagacja_do_przodu.png \"Propagacja do przodu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/suma_wazona.png \"Suma wazona\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, weights, biases):\n",
    "    # Pierwszy layer\n",
    "    z1 = np.dot(x, weights[0]) + biases[0]\n",
    "    result_activation_function = sigmoid(z1)\n",
    "    # Drugi layer\n",
    "    z2 = np.dot(result_activation_function, weights[1]) + biases[1]\n",
    "    result_activation_function2 = sigmoid(z2)\n",
    "\n",
    "    # Output layer\n",
    "    z3 = np.dot(result_activation_function2, weights[2]) + biases[2]\n",
    "    # output= sigmoid(z3)\n",
    "    # Użyje jednak funkcji softmax na końcu sieci\n",
    "    output = softmax(z3)\n",
    "\n",
    "\n",
    "    return result_activation_function, result_activation_function2, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doszlismy do końca sieci więc należy policzyć straty aby poprawić wyniki przewidywań. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, output_predict):\n",
    "    y_true = y_true.reshape(-1, 1) # Reshape danych do tablic 2D\n",
    "    output_predict = output_predict.reshape(-1, 1) # Reshape danych do tablic 2D\n",
    "    output_error = mean_squared_error(y_true, output_predict)\n",
    "    return output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zdecydowałem się zastasować średni błęd kwadratowy ponieważ jest to dość popularna metoda i używałem tego wcześniej do sprawdzenia straty przy przewidywań modelu Linii Regresyjnej."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function: Binary cross-entropy loss\n",
    "def compute_cost(y, y_hat):\n",
    "    m = y.shape[0]                            # Number of samples\n",
    "    y_hat = np.clip(y_hat, 1e-10, 1 - 1e-10)  # Avoid log(0) errors by clipping predictions\n",
    "\n",
    "    # Vectorized implementation of binary cross-entropy loss\n",
    "    cost = (-1 / m) * np.sum((y * np.log(y_hat)) + (1 - y) * np.log(1 - y_hat))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](images/backpropagation.png \"Backpropagation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def backward_propagation(x, output_error, weights, biases, learning_rate):\n",
    "\n",
    "#     weights = np.array(weights)\n",
    "#     biases = np.array(biases)\n",
    "\n",
    "#     input_error = np.dot(output_error, weights.T)\n",
    "#     weights_error = np.dot(x.T, output_error)\n",
    "\n",
    "#     print(weights.shape)\n",
    "#     print(weights_error.shape)\n",
    "#     print(output_error.shape)\n",
    "\n",
    "#     # Aktualizacja wag i baisow\n",
    "#     for i in range(len(weights) - 1):\n",
    "#         weights -= learning_rate * weights_error\n",
    "#         biases -= learning_rate * output_error \n",
    "\n",
    "#     return weights, biases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie mogłem tego zrobić na swój własny sposób, bo mi się kształty nie zgadzały."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(x, y, a1, a2, output, weights, biases, learning_rate):\n",
    "    m = y.shape[0]\n",
    "    error = output - y\n",
    "\n",
    "    # Obliczanie gradientow propagacji wstecznej dla wartwy wyjscia\n",
    "    dW3 = (1 / m) * np.dot(a2.T, error)\n",
    "    db3 = (1 / m) * np.sum(error, axis=0, keepdims=True)\n",
    "\n",
    "    # Obliczanie gradientu propagacji wstecznej dla pierwszej wartwy\n",
    "    dz2 = np.dot(error, weights[2].T) * (a2 * (1 - a2))\n",
    "    dW2 = (1 / m) * np.dot(a1.T, dz2)\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "\n",
    "    # Obliczanie gradientu propagacji wstecznej dla pierwszej wartwy\n",
    "    dz1 = np.dot(dz2, weights[1].T) * (a1 * (1 - a1))\n",
    "    dW1 = (1 / m) * np.dot(x.T, dz1)\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "\n",
    "    # Aktualizacja wag i biasów\n",
    "    weights[2] -= learning_rate * dW3\n",
    "    biases[2] -= learning_rate * db3\n",
    "    weights[1] -= learning_rate * dW2\n",
    "    biases[1] -= learning_rate * db2\n",
    "    weights[0] -= learning_rate * dW1\n",
    "    biases[0] -= learning_rate * db1\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nie potrafiłem sobie poradzić z implementacją propagacji wstecznej, dlatego użyłem kodu znalezionego na necie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracje/Epoki 0, Strata : 2.2594\n",
      "Wejscia: [0 0 0 1 1 0 1 1], Przewidywania sieci: [0.16187793 0.08814932 0.16177713 0.0880562  0.16194498 0.08822033\n",
      " 0.16184599 0.08812812], rzeczywiste wyniki [0 1 1 0]\n",
      "Iteracje/Epoki 1000, Strata : 2.2138\n",
      "Wejscia: [0 0 0 1 1 0 1 1], Przewidywania sieci: [0.12709967 0.11520068 0.13314945 0.12014929 0.12964056 0.11732973\n",
      " 0.13540259 0.12202804], rzeczywiste wyniki [0 1 1 0]\n",
      "Iteracje/Epoki 2000, Strata : 2.2096\n",
      "Wejscia: [0 0 0 1 1 0 1 1], Przewidywania sieci: [0.12061257 0.11995341 0.12676298 0.12584115 0.12485792 0.12402754\n",
      " 0.12949052 0.1284539 ], rzeczywiste wyniki [0 1 1 0]\n",
      "Iteracje/Epoki 3000, Strata : 2.2088\n",
      "Wejscia: [0 0 0 1 1 0 1 1], Przewidywania sieci: [0.1210817  0.12107438 0.12604673 0.12592335 0.1249894  0.1248938\n",
      " 0.12808074 0.1279099 ], rzeczywiste wyniki [0 1 1 0]\n",
      "Iteracje/Epoki 4000, Strata : 2.2087\n",
      "Wejscia: [0 0 0 1 1 0 1 1], Przewidywania sieci: [0.12170806 0.12173604 0.12582191 0.12578117 0.12514179 0.1251137\n",
      " 0.12738207 0.12731527], rzeczywiste wyniki [0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) \n",
    "y_predict = np.array([[0], [1], [1], [0]])  \n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 5000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Propagacja w przód\n",
    "    layer1, layer2, output = forward_propagation(x_train, weights, biases)\n",
    "\n",
    "    # Obliczenie kosztu straty po dojściu na koniec sieci\n",
    "    # loss = mse_loss(y_predict, output)\n",
    "    loss = compute_cost(y_predict, output)\n",
    "\n",
    "    # Propagacja wsteczna\n",
    "    update_weights, update_biases = backward_propagation(x_train, y_predict, layer1, layer2, output, weights, biases, learning_rate)\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Iteracje/Epoki {epoch}, Strata : {loss:.4f}\")\n",
    "        print(f\"Wejscia: {x_train.flatten()}, Przewidywania sieci: {output.flatten()}, rzeczywiste wyniki {y_predict.flatten()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podsumowanie: wyniki są beznadziejne, dalekie od dobrych przewidywań. Czy jest to spowodowane zanikiem spadku gradientu? Bo funkcja straty ma bardzo niewielkie spadki podczas większych iteracji. Jak by modelowi ciężko było zaktualizować lepsze wagi. Pamiętam, że czytałem taką informacje iż funkcja sigmoid ma tedencje do problemu zanikających gradientów, ale czy to na pewno to?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
